<h1>SETTING UP KAFKA CLUSTER</h1>

---

**Contents**:

- [Kafka architecture](#kafka-architecture)
  - [Stream processing vs. message broker](#stream-processing-vs-message-broker)
  - [Cluster, broker, topics, partition and messages](#cluster-broker-topics-partition-and-messages)
  - [Moving from ZooKeeper to KRaft](#moving-from-zookeeper-to-kraft)
    - [Basics](#basics)
    - [KRaft cluster](#kraft-cluster)
  - [Kafka Connect](#kafka-connect)
- [Installation and setup](#installation-and-setup)
  - [Prerequisites](#prerequisites)
  - [Docker Compose for Kafka cluster](#docker-compose-for-kafka-cluster)
  - [Docker Compose for Kafka Connect](#docker-compose-for-kafka-connect)
    - [Issues and fixes](#issues-and-fixes)
      - [Use of Kafka Connect 7.5.0 instead of latest version](#use-of-kafka-connect-750-instead-of-latest-version)
      - [Use of Docker file instead of in-line commands within Docker Compose](#use-of-docker-file-instead-of-in-line-commands-within-docker-compose)
  - [Docker Compose for Kafka initialiser (init-container pattern)](#docker-compose-for-kafka-initialiser-init-container-pattern)
  - [Docker Compose for Kafka UI](#docker-compose-for-kafka-ui)
  - [Docker networks and volumes to be added to Docker Compose](#docker-networks-and-volumes-to-be-added-to-docker-compose)

---

# Kafka architecture
> **Read about Kafka basics here**: [`pranav-gopalkrishna`/`learning-kafka`, **github.com**](https://github.com/pranav-gopalkrishna/learning-kafka)

## Stream processing vs. message broker
- Stream processing analyses and acts upon a continuous flow of data in real-time
- Message brokers store and forward individual messages between applications <br> **NOTE**: *Traditional message brokers delete them after processing*

---

Kafka uses message brokers for stream processing, but stream processing also includes:

- Real-time analytics
- Executing workflows
- Managing distributed data and computation

*Kafka does not delete messages after processing; the retention of messages is configurable.*

## Cluster, broker, topics, partition and messages
- Cluster: Allocated compute and memory for stream processing <br> **NOTE**: *A cluster can have one or more brokers*
- Broker: Server within a Kafka cluster that handles messages (data records) for topics:
    - Receives messages
    - Stores messages
    - Serves messages
- Topic: A logically defined and uniquely identified message queue
- Partition: A particular division of a topic queue
    - Generally, messages with the same key are grouped in the same partition
    - Partitions help organising messages in a topic and potentially make consumption efficient

## Moving from ZooKeeper to KRaft
### Basics
Raft:

- Informal acronym: "Reliable Available Fault-Tolerant"
- Consensus algorithm
- Esnures distributed systems agree upon single, shared state/sequence of events

KRaft:

- Internal metadata management mode
- Embeds Raft consensus protocol into Kafka brokers (for metadata management)
- Introduced to Kafka to replace ZooKeeper

**NOTE: Metadata for Kafka includes**: Cluster's state, including:

- Topics
- Partitions
- Replica assignments
- Configurations

> **References**:
> - [*KRaft: Apache Kafka Without ZooKeeper*, **developer.confluent.io**](https://developer.confluent.io/learn/kraft/)
> - [*KIP-500: Replace ZooKeeper with a Self-Managed Metadata Quorum*, **cwiki.apache.org/confluence**](https://cwiki.apache.org/confluence/display/KAFKA/KIP-500%3A+Replace+ZooKeeper+with+a+Self-Managed+Metadata+Quorum)

### KRaft cluster
A KRaft cluster requires a unique cluster ID. This ID:

- Must be the same across all nodes to function correctly
- Is generated during setup
- Is used by all brokers and controllers to identify the cluster
- Can be generated by:
    - Using the `kafka-storage.sh random-uuid` command
    - Then providing it in the configuration for all nodes

> **Reference**: [*Setup Kafka cluster with Kraft* by Saket Jain, **jainsaket-1994.medium.com**](https://jainsaket-1994.medium.com/setup-kafka-cluster-with-kraft-561f281b8e2a)

## Kafka Connect

# Installation and setup
## Prerequisites


## Docker Compose for Kafka cluster

```yaml
service:
  kafka:
    image: confluentinc/cp-kafka:7.8.0 # Image name (defaults to searching from Docker Hub)
    # REFERENCE: https://hub.docker.com/r/confluentinc/cp-kafka
    container_name: kafka
    ports:
      - "9092:9092"
      - "9093:9093"
    environment:
      # Configuration for cluster environment

      # KRaft parameters
      # REFERENCES:
      # - https://kafka.apache.org/documentation/#kraft
      # - https://docs.confluent.io/platform/current/installation/docker/config-reference.html
      KAFKA_NODE_ID: 1 # Unique identifier for this Kafka node; useful if there are multiple Kafka brokers
      KAFKA_PROCESS_ROLES: 'broker,controller' # Defines what this node does: broker (handles data/clients) + controller (manages cluster metadata)
      # NOTE: The above is "combined mode" - one node doing both jobs (perfect for development)
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka:9093' # List of nodes that vote on cluster decisions using Raft consensus; format: node_id@host:port
      # NOTE: This node votes on its own decisions since it's the only controller
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER' # Name of the network listener used for controller-to-controller communication
      # NOTE: Controllers use port 9093 with this listener to coordinate cluster state

      # Listeners
      # REFERENCES:
      # - https://docs.confluent.io/platform/current/installation/docker/config-reference.html
      # - https://kafka.apache.org/documentation/#brokerconfigs
      KAFKA_LISTENERS: 'PLAINTEXT://kafka:29092,PLAINTEXT_HOST://0.0.0.0:9092,CONTROLLER://kafka:9093' # This defines listeners by mapping listener names to host + port addresses
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092' # This makes Kafka accessible from outside of the container by advertising its location on the Docker host
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'

      # Cluster ID
      # REFERENCES:
      # - https://jainsaket-1994.medium.com/setup-kafka-cluster-with-kraft-561f281b8e2a
      # - https://docs.confluent.io/platform/current/installation/docker/config-reference.html#cp-kafka-example
      CLUSTER_ID: '2HplYgC_Sl2cG7XRPmu-hQ'
      # NOTE:
      # - Each KRaft cluster must have a unique cluster ID assigned to its CLUSTER_ID variable
      # - Must be a base64-encoded UUID (22 characters)
      # - A unique ID like this can be generated with the command: kafka-storage.sh random-uuid

      # Topic settings
      # REFERENCES:
      # - https://kafka.apache.org/documentation/#topicconfigs
      # - https://kafka.apache.org/documentation/#groupconfigs
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 # How many copies of consumer offset data to keep; 1 = single copy (okay for development, use 3 for production)
      # NOTE: Consumer offsets track "where you are" (like an index or seek position) in reading messages
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1 # Specifies the minimum number of ISRs, i.e. in-sync replicas (including the leader) required for a write to succeed when a producer sets acks to "all" (or "-1"); setting the value to k means at minimum k replicas must acknowledge the write (i.e. become "in-sync") for the write to succeed (check related: min.insync.replicas)
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1 # How many copies of transaction state to keep (check related: leader.replication.throttled.replicas)
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0 # The frequency with which the partition rebalance check is triggered by the controller (check related: leader.imbalance.check.interval.seconds)
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true' # Automatically create topics when first message is sent; true = convenient for development, false = better for production (explicit control)

      # Log settings
      KAFKA_LOG_DIRS: '/var/lib/kafka/data' # Where Kafka stores message data on disk; this directory is backed by Docker volume for persistence
      KAFKA_LOG_RETENTION_HOURS: # 168 After this, old messages are deleted; we can also use KAFKA_LOG_RETENTION_MS for milliseconds
      KAFKA_LOG_SEGMENT_BYTES: 1073741824 # Kafka splits logs into segments for easier management; smaller = more files and faster deletion, while larger = fewer files
    networks:
      - kafka-network  # Connects Kafka to a custom Docker network called kafka-network (defined at the bottom of this Docker Compose file under the heading networks)
      # NOTE:
      # - All services on this network can talk to each other by container name instead of IP addresses
      # - E.g.: A service can connect to kafka:29092 instead of using IP addresses
    volumes:
      # NOTE: This is crucial to ensure persistence of data across containers
      - kafka-data:/var/lib/kafka/data
    healthcheck: # REFERENCE: https://docs.docker.com/reference/compose-file/services/#healthcheck
      test: ["CMD-SHELL", "kafka-broker-api-versions --bootstrap-server localhost:9092 || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 30s
```
## Docker Compose for Kafka Connect
```yaml
service:
  kafka-connect:
    build:
      context: .
      dockerfile: Dockerfile.connect
    container_name: kafka-connect
    depends_on:
      kafka:
        condition: service_healthy
      postgres:
        condition: service_healthy
      kafka-init:
        condition: service_completed_successfully
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka:29092
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: "quickstart"
      CONNECT_CONFIG_STORAGE_TOPIC: "quickstart-config"
      CONNECT_OFFSET_STORAGE_TOPIC: "quickstart-offset"
      CONNECT_STATUS_STORAGE_TOPIC: "quickstart-status"
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
    ports:
      - "8083:8083"
    networks:
      - kafka-network
```

Referenced Dockerfile is available at: [`src/Dockerfile.connect`](../src/Dockerfile.connect)

### Issues and fixes
#### Use of Kafka Connect 7.5.0 instead of latest version
The following issue was faced when using the following configuration for the HTTP source connector:

```
curl --request POST \
  --url http://localhost:8083/connectors/ \
  --header 'Content-Type: application/json' \
  --data '
{
  "name": "http-source",
  "config": {
    "connector.class": "io.confluent.connect.http.HttpSourceConnector",
    "topic.name.pattern": "orders",
    "url": "http://data-source:3000/orders",
    "tasks.max": "1",
    "http.offset.mode": "SIMPLE_INCREMENTING",
    "http.initial.offset": "0",
    "key.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "confluent.topic.bootstrap.servers": "kafka:29092",
    "confluent.license": "",
    "confluent.topic.replication.factor": "1"
  }
}'
```

Then, the status was checked using:

```
curl --request GET \
  --url http://localhost:8083/connectors/http-source/status \
  --header 'Content-Type: application/json'
```

The response included:

```
{"name":"http-source-orders","connector":{"state":"FAILED","worker_id":"kafka-connect:8083","trace":"java.lang.NoSuchMethodError: 'org.apache.kafka.connect.util.TopicAdmin$NewTopicBuilder org.apache.kafka.connect.util.TopicAdmin$NewTopicBuilder.minInSyncReplicas(short) ...
```

`java.lang.NoSuchMethodError` indicated a version compatibility issue, which is why I downgraded the Kafka Connect version to 7.5.0 from whatever was the latest version. This led to another issue, which is discussed in the next section.

#### Use of Docker file instead of in-line commands within Docker Compose
**NOTE**: *The following was observed with Kafka Connect 7.5.0.*

Earlier, the following Docker Compose configuration was used:

```yaml
service:
  kafka-connect:
    image: confluentinc/cp-kafka-connect:7.5.0
    container_name: kafka-connect
    depends_on:
      kafka:
        condition: service_healthy
      postgres:
        condition: service_healthy
      kafka-init:
        condition: service_completed_successfully
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka:29092
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: "quickstart"
      CONNECT_CONFIG_STORAGE_TOPIC: "quickstart-config"
      CONNECT_OFFSET_STORAGE_TOPIC: "quickstart-offset"
      CONNECT_STATUS_STORAGE_TOPIC: "quickstart-status"
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
    ports:
      - "8083:8083"
    networks:
      - kafka-network
    command:
      - bash
      - -c
      - |
        echo "Installing HTTP source connector..."
        confluent-hub install --no-prompt confluentinc/kafka-connect-http-source:latest
        echo "Installing JDBC connector..."
        confluent-hub install --no-prompt confluentinc/kafka-connect-jdbc:latest
        echo "Starting Kafka Connect..."
        /etc/confluent/docker/run &
        wait
```

However, this seemed to produce silent failures in installation, that led to the following issue:

*HTTP source connector configuration*

```
curl --request POST \
  --url http://localhost:8083/connectors/ \
  --header 'Content-Type: application/json' \
  --data '
{
  "name": "http-source",
  "config": {
    "connector.class": "io.confluent.connect.http.HttpSourceConnector",
    "topic.name.pattern": "orders",
    "url": "http://data-source:3000/orders",
    "tasks.max": "1",
    "http.offset.mode": "SIMPLE_INCREMENTING",
    "http.initial.offset": "0",
    "key.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "confluent.topic.bootstrap.servers": "kafka:29092",
    "confluent.license": "",
    "confluent.topic.replication.factor": "1"
  }
}'
```

*Status check request*

```
curl --request GET \
  --url http://localhost:8083/connectors/http-source/status \
  --header 'Content-Type: application/json'
```

*Error (initial portion)*

```
{"error_code": 500, "message": "Failed to find any class that implements Connector and which name matches io.confluent.connect.http.HttpSourceConnector, available connectors are: ...
```

The error shows that the HTTP Source connector was not actually installed or Kafka Connect could not find it. The connector installation in the `command` section failed or the plugin path was not configured correctly. Hence, this seems like a runtime issue, and pre-installing the connector in the image was a potential solution to eliminate runtime installation issues. Hence, [`src/Dockerfile.connect`](../src/Dockerfile.connect) was used and the Docker Compose file was changed accordingly, and this resolved this issue.

## Docker Compose for Kafka initialiser (init-container pattern)
```yaml
service:
  # Kafka configuration (creates topics for Kafka running in 'kafka' container)
  kafka-init:
    image: confluentinc/cp-kafka:7.8.0
    container_name: kafka-init
    depends_on:
      kafka:
        condition: service_healthy
    entrypoint: ["/bin/bash", "-c"]
    command: |
      "
      echo 'Waiting for Kafka and Postgres to be ready...'

      sleep 5

      echo 'Creating Kafka Connect internal topics...'

      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic quickstart-config --partitions 1 --replication-factor 1 --config cleanup.policy=compact
      echo 'Created topic: quickstart-config'
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic quickstart-offset --partitions 25 --replication-factor 1 --config cleanup.policy=compact
      echo 'Created topic: quickstart-offset'
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic quickstart-status --partitions 5 --replication-factor 1 --config cleanup.policy=compact
      echo 'Created topic: quickstart-status'

      echo 'Creating Kafka topics...'
      
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic orders --partitions 3 --replication-factor 1 --config retention.ms=604800000
      echo 'Created topic: orders'
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic enriched_orders --partitions 3 --replication-factor 1 --config retention.ms=604800000
      echo 'Created topic: enriched_orders'
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic invalid_orders --partitions 3 --replication-factor 1 --config retention.ms=604800000
      echo 'Created topic: invalid_orders'

      echo 'Topics created successfully!'

      echo 'Ensuring enriched_orders table exists in Postgres...'
      apt-get update && apt-get install -y postgresql-client

      PGPASSWORD=kafkapass psql -h postgres -U kafkauser -d kafkadb -v ON_ERROR_STOP=1 <<'SQL'
      CREATE TABLE IF NOT EXISTS enriched_orders (
          order_id VARCHAR(255) PRIMARY KEY,
          product_name VARCHAR(255),
          quantity NUMERIC(10, 2),
          price NUMERIC(10, 2),
          order_date VARCHAR(50),
          total NUMERIC(10, 2),
          processed_at NUMERIC(20, 6),
          validator_version VARCHAR(50),
          validation_status VARCHAR(50),
          validation_message TEXT,
          created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
      );
      SQL

      echo 'Postgres schema ensured.'

      echo 'Listing all topics:'
      kafka-topics --bootstrap-server kafka:29092 --list
      
      echo 'Topic creation complete. Container will now exit.'
      "
    networks:
      - kafka-network
    restart: "no"
```

## Docker Compose for Kafka UI
```yaml
service:
  kafka-ui:
    image: provectuslabs/kafka-ui:latest # Image name (defaults to searching from Docker Hub)
    # REFERENCE: https://hub.docker.com/r/provectuslabs/kafka-ui
    container_name: kafka-ui
    depends_on:
      # REFERENCES:
      # - https://docs.docker.com/reference/compose-file/services/#depends_on
      # - https://docs.docker.com/reference/compose-file/services/#long-syntax-1
      # - https://docs.docker.com/reference/compose-file/services/#healthcheck
      kafka:
        condition: service_healthy # NOTE: Health is determined based on health tests defined for 'kafka' in its configuration (see its configuration above)
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local # Display name for the Kafka cluster in the UI; can be anything ("dev", "production", etc.)
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092 # Kafka UI connects to the 'kafka' container (over 'kafka-network' in our case); otherwise, the UI will look for Kafka within the container itself
      DYNAMIC_CONFIG_ENABLED: 'true' # Allows modifying Kafka configurations through the UI; making it as 'false' => UI is read-only
    networks:
      - kafka-network # See the corresponding field in the kafka configuration section
    healthcheck: # REFERENCE: https://docs.docker.com/reference/compose-file/services/#healthcheck
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:8080 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
```
## Docker networks and volumes to be added to Docker Compose
```yaml
networks:
  kafka-network:
    driver: bridge

volumes:
  # Defining volume names referenced above (their location paths are defined above)
  kafka-data:
  postgres-data:
```