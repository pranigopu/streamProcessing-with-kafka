services:
  kafka:
    image: confluentinc/cp-kafka:latest # Image name (defaults to searching from Docker Hub)
    # REFERENCE: https://hub.docker.com/r/confluentinc/cp-kafka
    container_name: kafka
    ports:
      - "9092:9092"
      - "9093:9093"
    environment:
      # Configuration for cluster environment

      # KRaft parameters
      # REFERENCES:
      # - https://kafka.apache.org/documentation/#kraft
      # - https://docs.confluent.io/platform/current/installation/docker/config-reference.html
      KAFKA_NODE_ID: 1 # Unique identifier for this Kafka node; useful if there are multiple Kafka brokers
      KAFKA_PROCESS_ROLES: 'broker,controller' # Defines what this node does: broker (handles data/clients) + controller (manages cluster metadata)
      # NOTE: The above is "combined mode" - one node doing both jobs (perfect for development)
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka:9093' # List of nodes that vote on cluster decisions using Raft consensus; format: node_id@host:port
      # NOTE: This node votes on its own decisions since it's the only controller
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER' # Name of the network listener used for controller-to-controller communication
      # NOTE: Controllers use port 9093 with this listener to coordinate cluster state

      # Listeners
      # REFERENCES:
      # - https://docs.confluent.io/platform/current/installation/docker/config-reference.html
      # - https://kafka.apache.org/documentation/#brokerconfigs
      KAFKA_LISTENERS: 'PLAINTEXT://kafka:29092,PLAINTEXT_HOST://0.0.0.0:9092,CONTROLLER://kafka:9093' # This defines listeners by mapping listener names to host + port addresses
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092' # This makes Kafka accessible from outside of the container by advertising its location on the Docker host
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'

      # Cluster ID
      # REFERENCES:
      # - https://jainsaket-1994.medium.com/setup-kafka-cluster-with-kraft-561f281b8e2a
      # - https://docs.confluent.io/platform/current/installation/docker/config-reference.html#cp-kafka-example
      CLUSTER_ID: '2HplYgC_Sl2cG7XRPmu-hQ'
      # NOTE:
      # - Each KRaft cluster must have a unique cluster ID assigned to its CLUSTER_ID variable
      # - Must be a base64-encoded UUID (22 characters)
      # - A unique ID like this can be generated with the command: kafka-storage.sh random-uuid

      # Topic settings
      # REFERENCES:
      # - https://kafka.apache.org/documentation/#topicconfigs
      # - https://kafka.apache.org/documentation/#groupconfigs
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 # How many copies of consumer offset data to keep; 1 = single copy (okay for development, use 3 for production)
      # NOTE: Consumer offsets track "where you are" (like an index or seek position) in reading messages
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1 # Specifies the minimum number of ISRs, i.e. in-sync replicas (including the leader) required for a write to succeed when a producer sets acks to "all" (or "-1"); setting the value to k means at minimum k replicas must acknowledge the write (i.e. become "in-sync") for the write to succeed (check related: min.insync.replicas)
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1 # How many copies of transaction state to keep (check related: leader.replication.throttled.replicas)
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0 # The frequency with which the partition rebalance check is triggered by the controller (check related: leader.imbalance.check.interval.seconds)
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true' # Automatically create topics when first message is sent; true = convenient for development, false = better for production (explicit control)

      # Log settings
      KAFKA_LOG_DIRS: '/var/lib/kafka/data' # Where Kafka stores message data on disk; this directory is backed by Docker volume for persistence
      KAFKA_LOG_RETENTION_HOURS: # 168 After this, old messages are deleted; we can also use KAFKA_LOG_RETENTION_MS for milliseconds
      KAFKA_LOG_SEGMENT_BYTES: 1073741824 # Kafka splits logs into segments for easier management; smaller = more files and faster deletion, while larger = fewer files
    networks:
      - kafka-network  # Connects Kafka to a custom Docker network called kafka-network (defined at the bottom of this Docker Compose file under the heading networks)
      # NOTE:
      # - All services on this network can talk to each other by container name instead of IP addresses
      # - E.g.: A service can connect to kafka:29092 instead of using IP addresses
    volumes:
      # NOTE: This is crucial to ensure persistence of data across containers
      - kafka-data:/var/lib/kafka/data
    healthcheck: # REFERENCE: https://docs.docker.com/reference/compose-file/services/#healthcheck
      test: ["CMD-SHELL", "kafka-broker-api-versions --bootstrap-server localhost:9092 || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 30s
  
  kafka-ui:
    image: provectuslabs/kafka-ui:latest # Image name (defaults to searching from Docker Hub)
    # REFERENCE: https://hub.docker.com/r/provectuslabs/kafka-ui
    container_name: kafka-ui
    depends_on:
      # REFERENCES:
      # - https://docs.docker.com/reference/compose-file/services/#depends_on
      # - https://docs.docker.com/reference/compose-file/services/#long-syntax-1
      # - https://docs.docker.com/reference/compose-file/services/#healthcheck
      kafka:
        condition: service_healthy # NOTE: Health is determined based on health tests defined for 'kafka' in its configuration (see its configuration above)
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local # Display name for the Kafka cluster in the UI; can be anything ("dev", "production", etc.)
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092 # Kafka UI connects to the 'kafka' container (over 'kafka-network' in our case); otherwise, the UI will look for Kafka within the container itself
      DYNAMIC_CONFIG_ENABLED: 'true' # Allows modifying Kafka configurations through the UI; making it as 'false' => UI is read-only
    networks:
      - kafka-network # See the corresponding field in the kafka configuration section
    healthcheck: # REFERENCE: https://docs.docker.com/reference/compose-file/services/#healthcheck
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:8080 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s

  postgres:
    image: postgres:16-alpine # Image name (defaults to searching from Docker Hub)
    container_name: postgres
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: kafkauser
      POSTGRES_PASSWORD: kafkapass
      POSTGRES_DB: kafkadb
      PGDATA: /var/lib/postgresql/data/pgdata # Location within the host to store database data
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - kafka-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U kafkauser -d kafkadb"]
      interval: 10s
      timeout: 5s
      retries: 5

  jumphost:
    build:
      context: . # Defines either a path to a directory containing a Dockerfile, or a URL to a Git repository
      # NOTE: When the value supplied is a relative path, it is interpreted as relative to the project directory; compose warns you about the absolute path used to define the build context as those prevent the Compose file from being portable
      # REFERENCE: https://docs.docker.com/reference/compose-file/build/#context
      dockerfile: Dockerfile.jumphost # Sets an alternate Dockerfile (a relative path is resolved from the build context); defaults to 'Dockerfile' if not specified
      # REFERENCE: https://docs.docker.com/reference/compose-file/build/#dockerfile
    image: jumphost:latest # Name to tag the built image (format = image_name:version_tag)
    container_name: jumphost
    depends_on:
      - kafka
      - postgres
    command: sleep infinity
    networks:
      - kafka-network
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_DB: kafkadb
      POSTGRES_USER: kafkauser
      POSTGRES_PASSWORD: kafkapass
    stdin_open: true
    tty: true
    volumes:
      - ./scripts:/scripts
      - jumphost-home:/root # Persist home directory (bashrc, aliases, etc.)
      - jumphost-data:/data # Persist data directory (JSON Server db)
  
  # Kafka configuration (creates topics for Kafka running in 'kafka' container)
  kafka-init:
    image: confluentinc/cp-kafka:latest
    container_name: kafka-init
    depends_on:
      kafka:
        condition: service_healthy
      postgres:
        condition: service_healthy
    entrypoint: ["/bin/bash", "-c"]
    command: |
      "
      echo 'Waiting for Kafka and Postgres to be ready...'

      sleep 5

      echo 'Creating Kafka Connect internal topics...'

      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic quickstart-config --partitions 1 --replication-factor 1 --config cleanup.policy=compact
      echo 'Created topic: quickstart-config'
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic quickstart-offset --partitions 25 --replication-factor 1 --config cleanup.policy=compact
      echo 'Created topic: quickstart-offset'
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic quickstart-status --partitions 5 --replication-factor 1 --config cleanup.policy=compact
      echo 'Created topic: quickstart-status'

      echo 'Creating Kafka topics...'
      
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic orders --partitions 3 --replication-factor 1 --config retention.ms=604800000
      echo 'Created topic: orders'
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic enriched_orders --partitions 3 --replication-factor 1 --config retention.ms=604800000
      echo 'Created topic: enriched_orders'
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic invalid_orders --partitions 3 --replication-factor 1 --config retention.ms=604800000
      echo 'Created topic: invalid_orders'

      echo 'Topics created successfully!'

      echo 'Ensuring enriched_orders table exists in Postgres...'
      apt-get update && apt-get install -y postgresql-client

      PGPASSWORD=kafkapass psql -h postgres -U kafkauser -d kafkadb -v ON_ERROR_STOP=1 <<'SQL'
      CREATE TABLE IF NOT EXISTS enriched_orders (
          order_id VARCHAR(255) PRIMARY KEY,
          product_name VARCHAR(255),
          quantity NUMERIC(10, 2),
          price NUMERIC(10, 2),
          order_date VARCHAR(50),
          id VARCHAR(50),
          total_price NUMERIC(10, 2),
          created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
      );
      SQL

      echo 'Postgres schema ensured.'

      echo 'Listing all topics:'
      kafka-topics --bootstrap-server kafka:29092 --list
      
      echo 'Topic creation complete. Container will now exit.'
      "
    networks:
      - kafka-network
    restart: "no"
  
  order-validator:
    build:
      context: .
      dockerfile: Dockerfile.validator
    image: order-validator:latest
    container_name: order-validator
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - kafka-network
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      ORDERS_TOPIC: orders
      ENRICHED_ORDERS_TOPIC: enriched_orders
      INVALID_ORDERS_TOPIC: invalid_orders
      CONSUMER_GROUP: order-validator
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import sys; sys.exit(0)"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
  
  data-source:
    build:
      context: . # Defines either a path to a directory containing a Dockerfile, or a URL to a Git repository
      # NOTE: When the value supplied is a relative path, it is interpreted as relative to the project directory; compose warns you about the absolute path used to define the build context as those prevent the Compose file from being portable
      # REFERENCE: https://docs.docker.com/reference/compose-file/build/#context
      dockerfile: Dockerfile.datasource # Sets an alternate Dockerfile (a relative path is resolved from the build context); defaults to 'Dockerfile' if not specified
      # REFERENCE: https://docs.docker.com/reference/compose-file/build/#dockerfile
    image: data-source:latest # Name to tag the built image (format = image_name:version_tag)
    container_name: data-source
    ports:
      - "3000:3000"
    networks:
      - kafka-network
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_DB: kafkadb
      POSTGRES_USER: kafkauser
      POSTGRES_PASSWORD: kafkapass
    stdin_open: true
    tty: true
    volumes:
      - data-source-home:/root # Persist home directory (bashrc, aliases, etc.)
      - data-source-data:/data # Persist data directory (JSON Server)
      - ./data/raw_orders.json:/data/raw_orders.json # NOTE: ro at the end means "read-only"

  kafka-connect:
    build:
      context: .
      dockerfile: Dockerfile.connect
    container_name: kafka-connect
    depends_on:
      kafka:
        condition: service_healthy
      postgres:
        condition: service_healthy
      kafka-init:
        condition: service_completed_successfully
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka:29092
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: "quickstart"
      CONNECT_CONFIG_STORAGE_TOPIC: "quickstart-config"
      CONNECT_OFFSET_STORAGE_TOPIC: "quickstart-offset"
      CONNECT_STATUS_STORAGE_TOPIC: "quickstart-status"
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
    ports:
      - "8083:8083"
    networks:
      - kafka-network

networks:
  kafka-network:
    driver: bridge

volumes:
  # Defining volume names referenced above (their location paths are defined above)
  kafka-data:
  postgres-data:
  jumphost-home:
  jumphost-data:
  data-source-home:
  data-source-data: